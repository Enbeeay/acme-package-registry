#!/usr/bin/env python3
import os
import sys
import re
import subprocess
import logging
import csv
import typer
from typing import Optional
import json

from src.logging_config import setup_logging
from src.metrics.helpers.pull_model import pull_model_info, UrlType, get_url_type, canonicalize_hf_url
from src.orchestrator import calculate_all_metrics

app = typer.Typer(add_completion=False)

# --- Setup Python path for module resolution ---
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- CLI File Dispatch ---
def _dispatch_file_arg():
    """If user runs script with a path (e.g., ./run urls.txt), auto-dispatch to process()."""
    if len(sys.argv) >= 2:
        first = sys.argv[1]
        candidate = first if os.path.isabs(first) else os.path.join(os.getcwd(), first)
        if os.path.isfile(candidate):
            sys.argv = [sys.argv[0], "process", candidate]


# --- Commands ---

@app.command()
def install():
    """Install Python dependencies from requirements.txt."""
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])
        typer.echo("Dependencies installed successfully.")
        raise typer.Exit(0)
    except subprocess.CalledProcessError as e:
        typer.echo(f"Install failed: {e}", err=True)
        raise typer.Exit(1)


@app.command()
def test():
    """Run pytest and report results with test count and coverage."""
    try:
        result = subprocess.run(
            [sys.executable, "-m", "pytest", "--cov=src", "--cov-report=term-missing"],
            capture_output=True,
            text=True,
        )

        # Print test output
        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(result.stderr, file=sys.stderr)

        # Parse totals safely
        passed = 0
        total = 0
        coverage = 0
        m_total = re.search(r"collected\s+(\d+)\s+items", result.stdout)
        if m_total:
            total = int(m_total.group(1))
        m_passed = re.search(r"=+\s*(\d+)\s+passed", result.stdout)
        if m_passed:
            passed = int(m_passed.group(1))
        m_cov = re.search(r"^TOTAL\s+\d+\s+\d+\s+(\d+)%$", result.stdout, re.MULTILINE)
        if m_cov:
            coverage = int(m_cov.group(1))

        print(f"{passed}/{total} test cases passed. {coverage}% line coverage achieved.")
    except Exception as e:
        typer.echo(f"Test failed: {e}", err=True)
        raise typer.Exit(1)
    # Important: propagate pytest exit status exactly
    raise typer.Exit(result.returncode)


@app.command(name="process")
def process_cmd(url_file: str = typer.Argument(..., help="Path to file with URLs (code_link,dataset_link,model_link per line)")):
    """Process a file where each line has comma-separated links: code_link, dataset_link, model_link.

    - code_link can be blank and is not processed (future use)
    - dataset_link can be blank; when blank, we try to infer dataset(s) from the model card/README
    - we maintain a run-scoped set of encountered dataset IDs and log when a model shares an already seen dataset
    - only model_link rows are evaluated for metrics; NDJSON is printed per model
    """
    # Enforce LOG_FILE rule: only write to an existing file (if LOG_FILE is provided)
    log_path: Optional[str] = os.environ.get("LOG_FILE")
    if log_path:
        if not os.path.exists(log_path):
            typer.echo(f"Error: LOG_FILE does not exist: {log_path}", err=True)
            raise typer.Exit(1)
    # configure logging only after the existence check
    setup_logging()
    logging.debug("process_cmd: logging configured")
    logging.info(f"Starting processing for URL file: {url_file}")

    encountered_datasets: set[str] = set()

    try:
        with open(url_file, "r", newline="") as f:
            reader = csv.reader(f)
            rows = [row for row in reader if row and any(cell.strip() for cell in row)]
    except FileNotFoundError:
        logging.error(f"URL file not found: {url_file}")
        typer.echo(f"Error: URL file not found at {url_file}", err=True)
        raise typer.Exit(1)
    except Exception as e:
        logging.error(f"Error reading URL file {url_file}: {e}")
        typer.echo(f"Error reading URL file: {e}", err=True)
        raise typer.Exit(1)

    for idx, raw in enumerate(rows, start=1):
        # Normalize to 3 columns
        code_link = raw[0].strip() if len(raw) > 0 else ""
        dataset_link = raw[1].strip() if len(raw) > 1 else ""
        model_link = raw[2].strip() if len(raw) > 2 else ""

        # Record dataset if explicitly provided
        if dataset_link:
            if get_url_type(dataset_link) == UrlType.HUGGING_FACE_DATASET:
                try:
                    dataset_id = dataset_link.split("/datasets/")[1].split("?")[0].split("#")[0]
                    encountered_datasets.add(dataset_id)
                    logging.info(f"[line {idx}] Registered dataset from link: {dataset_id}")
                except Exception:
                    logging.warning(f"[line {idx}] Could not parse dataset id from link: {dataset_link}")
            else:
                logging.warning(f"[line {idx}] Non-dataset URL provided in dataset column: {dataset_link}")

        # Validate and process model link
        if not model_link:
            logging.warning(f"[line {idx}] Missing model link; skipping line.")
            continue

        # Normalize HF model URLs (strip /tree/main etc.) before typing
        model_link = canonicalize_hf_url(model_link)
        url_type = get_url_type(model_link)
        if url_type != UrlType.HUGGING_FACE_MODEL:
            logging.warning(f"[line {idx}] Skipping non-model URL in model column: {model_link} (type: {url_type.name})")
            continue

        logging.info(f"[line {idx}] Processing model: {model_link}")

        try:
            model_info = pull_model_info(model_link)
            if not model_info:
                logging.warning(f"[line {idx}] Could not retrieve info for model URL: {model_link}")
                continue

            # Infer datasets if none explicitly given on this line
            inferred_shared = []
            discovered = []
            try:
                # 1) Structured signal via model cardData
                card = getattr(model_info, "cardData", None)
                ds_list = []
                if isinstance(card, dict):
                    val = card.get("datasets")
                    if isinstance(val, (list, tuple)):
                        ds_list = [str(x) for x in val if x]
                    elif isinstance(val, str):
                        ds_list = [val]

                # 2) Unstructured signal via README content
                readme_ds: list[str] = []
                try:
                    from src.metrics.dataset_code_avail import _fetch_readme_content  # type: ignore
                    readme_text = _fetch_readme_content(model_info) or ""
                    # Find patterns like "/datasets/<org>/<name>" or "/datasets/<name>"
                    pat = r"/datasets/([\w\-]+(?:/[\w\-]+)?)"
                    readme_ds = re.findall(pat, readme_text)
                except Exception:
                    # README fetching may fail offline; that's fine
                    readme_ds = []

                all_candidates = []
                # Maintain insertion order without duplicates
                seen_tmp = set()
                for ds in list(ds_list) + list(readme_ds):
                    if ds and ds not in seen_tmp:
                        all_candidates.append(ds)
                        seen_tmp.add(ds)

                for ds in all_candidates:
                    if ds in encountered_datasets:
                        inferred_shared.append(ds)
                    else:
                        encountered_datasets.add(ds)
                        discovered.append(ds)

                if inferred_shared:
                    logging.info(f"[line {idx}] Model shares already encountered dataset(s): {', '.join(inferred_shared)}")
                if discovered:
                    logging.info(f"[line {idx}] Discovered new dataset(s) from README/card: {', '.join(discovered)}")
            except Exception as infer_err:
                logging.debug(f"[line {idx}] Dataset inference failed: {infer_err}")

            # Compute and emit NDJSON for this model
            ndjson_output = calculate_all_metrics(model_info, model_link)
            # Post-process for autograder: convert "org/name" -> "name" for MODEL outputs
            try:
                obj = json.loads(ndjson_output)
                if obj.get("category") == "MODEL":
                    name = obj.get("name", "")
                    if isinstance(name, str) and "/" in name:
                        obj["name"] = name.split("/")[-1]
                print(json.dumps(obj))
            except Exception:
                # If anything goes odd, fall back to original
                print(ndjson_output)
            logging.debug(f"[line {idx}] Emitted NDJSON for {model_info.id}")
        except ValueError as ve:
            logging.error(f"[line {idx}] Value error for model URL {model_link}: {ve}")
            typer.echo(f"Error processing URL {model_link}: {ve}", err=True)
        except Exception as e:
            logging.error(f"[line {idx}] Unexpected error for URL {model_link}: {e}", exc_info=True)
            typer.echo(f"Unexpected error for URL {model_link}: {e}", err=True)

    logging.info("Finished processing all URLs.")
    raise typer.Exit(0)


# --- Entry Point ---

_dispatch_file_arg()

if __name__ == "__main__":
    app()